Building a SLU for the PTIcs domain
===================================

Available data
--------------

At this moment there we have only data which were automatically generated using handcrafted SLU (HDC SLU) parser on the
transcribed audio. In general, the quality of the automatic annotation is very good.

The data can be prepared using the ``prapare_data.py`` script. It assumes that there exist the ``indomain_data`` directory
with links to directories with ``asr_transcribed.xml`` files. Then it uses these files to extract transcriptions
and generate automatic transcriptions using the PTICSHDCSLU parser from the ``hdc_slu.py`` file.

The script generates the following files:

- ``*.trn``: contains manual transcriptions
- ``*.trn.hdc.sem``: contains automatic annotation from transcriptions using handcrafted SLU
- ``*.asr``: contains ASR 1-best results
- ``*.asr.hdc.sem``: contains automatic annotation from 1-best ASR using handcrafted SLU
- ``*.nbl``: contains ASR N-best results
- ``*.nbl.hdc.sem``: contains automatic annotation from n-best ASR using handcrafted SLU

The scripts accepts ``uniq`` parameter for fast generation of HDC SLU only transcriptions. It generates only HDC SLU
output only for unique transcriptions. This is useful when tuning the HDC SLU.

Building the models
-------------------

First, prepare the data. Link the directories with the domain data into the ``indomain_data`` directory. Then run the
following command:

::

    ./prepare_data.py


Second, train and test the models.

::

    ./train.py && ./test.py && ./test_bootstrap.py

Third, look at the *.score files or compute the interesting scores by

::

    ./print_scores.sh


Future work
-----------

- The ``prepare_data.py`` will have to use ASR, NBLIST, and CONFNET data generated by the latest ASR system instead of the
  logged ASR results because the ASR can change over time.
- Condition the SLU DialogueActItem decoding on the previous system dialogue act.


Evaluation
----------

Evaluation of ASR from the call logs files
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The current ASR performance computed on from the call logs is as follows:
::

    Please note that the scoring is implicitly ignoring all non-speech events.

    Ref: all.trn
    Tst: all.asr
    |==============================================================================================|
    |            | # Sentences  |  # Words  |   Corr   |   Sub    |   Del    |   Ins    |   Err    |
    |----------------------------------------------------------------------------------------------|
    | Sum/Avg    |     2455     |   6072    |  55.02   |  16.78   |  28.19   |   1.45   |  46.43   |
    |==============================================================================================|


The used ASR decoder for the first 2455 sentences is Google.

Evaluation of the min number of feature counts
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Probably due to small amount of data (about 2000 utterances), it is still beneficial to use features which appeared in
the training data only two times.


Cheating experiment: train and test on all data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Due to sparsity issue, the evaluation on proper test and dev sets suffers from sampling errors. Therefore, here
we presents results when all data are used as training data and the metrics are evaluated on the training data!!!

Using the ``./print_scores.sh`` one can get:

::

    ----------------------------------------------------------------------------------------------------------
    DATA ALL ASR - HDC SLU
    ----------------------------------------------------------------------------------------------------------
    Ref: all.trn.hdc.sem
    Tst: all.asr.hdc.sem
    The results are based on 2455 DAs
    Total precision:  45.49
    Total recall:     43.98
    Total F-measure:  44.72
    ----------------------------------------------------------------------------------------------------------
    DATA ALL ASR - ASR model
    ----------------------------------------------------------------------------------------------------------
    Ref: all.trn.hdc.sem
    Tst: all.asr.model.asr.sem.out
    The results are based on 2455 DAs
    Total precision:  76.28
    Total recall:     74.44
    Total F-measure:  75.35

    ----------------------------------------------------------------------------------------------------------
    DATA ALL NBL - HDC SLU
    ----------------------------------------------------------------------------------------------------------
    Ref: all.trn.hdc.sem
    Tst: all.nbl.hdc.sem
    The results are based on 2455 DAs
    Total precision:  40.99
    Total recall:     39.49
    Total F-measure:  40.23
    ----------------------------------------------------------------------------------------------------------
    DATA ALL NBL - NBL model
    ----------------------------------------------------------------------------------------------------------
    Ref: all.trn.hdc.sem
    Tst: all.nbl.model.nbl.sem.out
    The results are based on 2455 DAs
    Total precision:  78.36
    Total recall:     78.42
    Total F-measure:  78.39

    ----------------------------------------------------------------------------------------------------------
    DATA ALL TRN - HDC SLU
    ----------------------------------------------------------------------------------------------------------
    Ref: all.trn.hdc.sem
    Tst: all.trn.hdc.sem
    The results are based on 2455 DAs
    Total precision: 100.00
    Total recall:    100.00
    Total F-measure: 100.00
    ----------------------------------------------------------------------------------------------------------
    DATA ALL TRN - TRN model
    ----------------------------------------------------------------------------------------------------------
    Ref: all.trn.hdc.sem
    Tst: all.trn.model.trn.sem.out
    The results are based on 2455 DAs
    Total precision:  98.62
    Total recall:     98.51
    Total F-measure:  98.57

If the automatic annotations were correct, we could conclude that the F-measure of the HDC SLU parser on 1-best
is about 44.72 and on N-best is about 40.23 %. This is confusing as it looks like that the decoding from n-best lists
gives worse results when compared to decoding from 1-best ASR hypothesis.

Evaluation of TRN model on test data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The TRN model is trained on transcriptions and evaluated on transcriptions from test data.

::

    ----------------------------------------------------------------------------------------------------------
    DATA TEST TRN - HDC SLU
    ----------------------------------------------------------------------------------------------------------
    Ref: test.trn.hdc.sem
    Tst: test.trn.hdc.sem
    The results are based on 246 DAs
    Total precision: 100.00
    Total recall:    100.00
    Total F-measure: 100.00
    ----------------------------------------------------------------------------------------------------------
    DATA TEST TRN - TRN model
    ----------------------------------------------------------------------------------------------------------
    Ref: test.trn.hdc.sem
    Tst: test.trn.model.trn.sem.out
    The results are based on 246 DAs
    Total precision:  98.07
    Total recall:     97.32
    Total F-measure:  97.69

One can see that the performance of the TRN model on TRN test data is **NOT** 100 % perfect. This is probably due to
the mismatch between the train and test data sets. Once more training data will be available, we can expect better
results.

Evaluation of ASR model on test data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ASR model is trained on 1-best ASR output and evaluated on the 1-best ASR output from test data.

::

    ----------------------------------------------------------------------------------------------------------
    DATA TEST ASR - HDC SLU
    ----------------------------------------------------------------------------------------------------------
    Ref: test.trn.hdc.sem
    Tst: test.asr.hdc.sem
    The results are based on 246 DAs
    Total precision:  45.56
    Total recall:     45.21
    Total F-measure:  45.38
    ----------------------------------------------------------------------------------------------------------
    DATA TEST ASR - ASR model
    ----------------------------------------------------------------------------------------------------------
    Ref: test.trn.hdc.sem
    Tst: test.asr.model.asr.sem.out
    The results are based on 246 DAs
    Total precision:  67.98
    Total recall:     65.90
    Total F-measure:  66.93

The **ASR model scores significantly better** on the ASR test data when compared to *the HDC SLU parser* when evaluated
on the ASR data. The improvement is about 20 % in F-measure (absolute). This shows that SLU trained on the ASR data
can be beneficial.

Evaluation of NBL model on test data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The NBL model is trained on N-best ASR output and evaluated on the N-best ASR from test data.

::

    ----------------------------------------------------------------------------------------------------------
    DATA TEST NBL - HDC SLU
    ----------------------------------------------------------------------------------------------------------
    Ref: test.trn.hdc.sem
    Tst: test.nbl.hdc.sem
    The results are based on 246 DAs
    Total precision:  41.31
    Total recall:     41.00
    Total F-measure:  41.15
    ----------------------------------------------------------------------------------------------------------
    DATA TEST NBL - NBL model
    ----------------------------------------------------------------------------------------------------------
    Ref: test.trn.hdc.sem
    Tst: test.nbl.model.nbl.sem.out
    The results are based on 246 DAs
    Total precision:  68.48
    Total recall:     67.43
    Total F-measure:  67.95

One can see that using nblists even from Google ASR can help; though only a little (about 1 %). When more data will be
available, more test and more feature engineering can be done. However, we are more interested in extracting features
from lattices or confusion networks. Now, we have to wait for a working decoder generating *good* lattices.
The OpenJulius decoder is not a suitable as it crashes unexpectedly and cannot be used in a real system.